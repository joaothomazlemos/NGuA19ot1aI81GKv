{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development and evaluation\n",
    "\n",
    "The objective of this notebook is to engineer ml models and test against preprocessed data to gather the best f1 score metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of candidates:\n",
    "\n",
    "1 - RandomForestClassifier\n",
    "2 - XGBClassifier\n",
    "3 - RidgeClassifier\n",
    "4 - SVC\n",
    "5  - KNeighborsClassifier\n",
    "6 - LogisticRegression\n",
    "7 - DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Importing libraries---------#\n",
    "\n",
    "#---Data analysis---#\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#---Data splitting---#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#---evaluation---#\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "#---visualization---#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#---utils---#\n",
    "import os\n",
    "\n",
    "#---data---#\n",
    "df_test = pd.read_csv('../data/preprocessed/df_test.csv')\n",
    "df_train = pd.read_csv('../data/preprocessed/df_train.csv')\n",
    "\n",
    "\n",
    "#---------Models---------#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (4510, 11)\n",
      "Test data shape:  (7668, 11)\n"
     ]
    }
   ],
   "source": [
    "#dataset shapes\n",
    "print(\"Train data shape: \", df_train.shape)\n",
    "print(\"Test data shape: \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "X_test = df_test.drop('y', axis=1)\n",
    "y_test = df_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix and vector data undersampled\n",
    "\n",
    "X = df_train.drop('y', axis=1)\n",
    "y = df_train['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest parameters\n",
    "random_forest_params = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "xgboost_params = {\n",
    "    'n_estimators': [100, 200, 400, 800, 1600],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [2, 4, 8, 16, 32],\n",
    "    'gamma': [0, 0.1, 0.5, 1.0],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'objective': ['binary:logistic'],\n",
    "    'eval_metric': ['logloss']\n",
    "}\n",
    "\n",
    "# Ridge classifier parameters\n",
    "ridge_params = {\n",
    "    'alpha': [0.1, 1.0, 10.0],\n",
    "    \n",
    "    'fit_intercept': [True, False],\n",
    "    'solver': [  'cholesky', 'lsqr', 'sparse_cg',  'lbfgs'],\n",
    "    'positive': [True]\n",
    "}\n",
    "\n",
    "# Support vector classifier parameters\n",
    "svc_params = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]\n",
    "    \n",
    "}\n",
    "\n",
    "# Logistic regression parameters\n",
    "logistic_params = {\n",
    "    'penalty': ['l2', 'none'],\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'fit_intercept': [True, False],\n",
    "    'solver': ['newton-cg','liblinear'],\n",
    "    'l1_ratio': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# K-nearest neighbors parameters\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [10, 30, 50],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# decision tree parameters\n",
    "decision_tree_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [3, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [3, 10, 20],\n",
    "    'min_impurity_decrease': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "\n",
    "# Create a list of tuples where each tuple contains (model, parameter_grid)\n",
    "models_and_params = [\n",
    "    (LogisticRegression(), logistic_params),\n",
    "    (KNeighborsClassifier(), knn_params),\n",
    "    (DecisionTreeClassifier(), decision_tree_params),\n",
    "    (RandomForestClassifier(), random_forest_params),\n",
    "    (XGBClassifier(), xgboost_params),\n",
    "\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LogisticRegression\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1171: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'solver': 'newton-cg', 'penalty': 'l2', 'l1_ratio': 0.0, 'fit_intercept': False, 'C': 0.1}\n",
      "Best estimator:  LogisticRegression(C=0.1, fit_intercept=False, l1_ratio=0.0, solver='newton-cg')\n",
      "Best F1 Score on validation set: 81.64%\n",
      "----------------------------------------------------------\n",
      "Model: KNeighborsClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 7, 'leaf_size': 10, 'algorithm': 'ball_tree'}\n",
      "Best estimator:  KNeighborsClassifier(algorithm='ball_tree', leaf_size=10, n_neighbors=7)\n",
      "Best F1 Score on validation set: 77.79%\n",
      "----------------------------------------------------------\n",
      "Model: DecisionTreeClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'splitter': 'random', 'min_samples_split': 2, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.01, 'max_leaf_nodes': 3, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "Best estimator:  DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features='log2',\n",
      "                       max_leaf_nodes=3, min_impurity_decrease=0.01,\n",
      "                       splitter='random')\n",
      "Best F1 Score on validation set: 35.98%\n",
      "----------------------------------------------------------\n",
      "Model: RandomForestClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'gini', 'bootstrap': True}\n",
      "Best estimator:  RandomForestClassifier(max_depth=10, min_samples_leaf=2)\n",
      "Best F1 Score on validation set: 85.83%\n",
      "----------------------------------------------------------\n",
      "Model: XGBClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'subsample': 0.5, 'objective': 'binary:logistic', 'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.01, 'gamma': 0, 'eval_metric': 'logloss'}\n",
      "Best estimator:  XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=0, gpu_id=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=800, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
      "Best F1 Score on validation set: 85.93%\n",
      "----------------------------------------------------------\n",
      "Model: RidgeClassifier\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Parameters: {'solver': 'lbfgs', 'positive': True, 'fit_intercept': True, 'alpha': 1.0}\n",
      "Best estimator:  RidgeClassifier(positive=True, solver='lbfgs')\n",
      "Best F1 Score on validation set: 76.51%\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "45 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1446, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 826, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='cholesky' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1446, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 826, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='sparse_cg' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 1446, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight)\n",
      "  File \"c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py\", line 826, in fit\n",
      "    raise ValueError(\n",
      "ValueError: solver='lsqr' does not support positive fitting. Please set the solver to 'auto' or 'lbfgs', or set `positive=False`\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\joao-lemos\\AppData\\Local\\anaconda3\\envs\\jao\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [      nan       nan       nan       nan 0.7650901       nan       nan\n",
      "       nan       nan       nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#list of the candidates models with best hyperparameters for each \n",
    "best_models = []\n",
    "scores = []\n",
    "# Iterate through models and parameters\n",
    "for model, param_grid in models_and_params:\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Create a StratifiedKFold cross-validation strategy\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # good for representing the data set equally for each class as\n",
    "                                                                    # they are imbalanced ( new data in the future per se)\n",
    "    \n",
    "    # Create a custom scorer using f1_score\n",
    "    custom_scorer = make_scorer(f1_score)\n",
    "    \n",
    "    # Create GridSearchCV\n",
    "    grid_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        scoring=custom_scorer,\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        n_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(X, y)  # X is your feature matrix, y is your target vector\n",
    "    \n",
    "    # Print the best parameters and F1 score\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print('Best estimator: ', grid_search.best_estimator_)\n",
    "    print(\"Best F1 Score on validation set: {:.2%}\".format(grid_search.best_score_))\n",
    "    best_models.append(grid_search.best_estimator_)\n",
    "    scores.append(grid_search.best_score_)\n",
    "    print('----------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, fit_intercept=False, l1_ratio=0.0, solver='newton-cg')\n",
      "------------------\n",
      "Test score:  0.837376108502869\n",
      "------------------ \n",
      "\n",
      "KNeighborsClassifier(algorithm='ball_tree', leaf_size=10, n_neighbors=7)\n",
      "------------------\n",
      "Test score:  0.7952529994783516\n",
      "------------------ \n",
      "\n",
      "DecisionTreeClassifier(criterion='entropy', max_depth=10, max_features='log2',\n",
      "                       max_leaf_nodes=3, min_impurity_decrease=0.01,\n",
      "                       splitter='random')\n",
      "------------------\n",
      "Test score:  0.9299687010954617\n",
      "------------------ \n",
      "\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=2)\n",
      "------------------\n",
      "Test score:  0.8495044340114762\n",
      "------------------ \n",
      "\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='logloss',\n",
      "              feature_types=None, gamma=0, gpu_id=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, n_estimators=800, n_jobs=None,\n",
      "              num_parallel_tree=None, predictor=None, random_state=None, ...)\n",
      "------------------\n",
      "Test score:  0.847678664580073\n",
      "------------------ \n",
      "\n",
      "RidgeClassifier(positive=True, solver='lbfgs')\n",
      "------------------\n",
      "Test score:  0.8783255086071987\n",
      "------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = []\n",
    "\n",
    "for model in best_models:\n",
    "    print(model)\n",
    "    print('------------------')\n",
    "    print('Test score: ', model.score(X_test, y_test))\n",
    "    print('------------------ \\n')\n",
    "    test_scores.append(model.score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "KNeighborsClassifier\n",
      "DecisionTreeClassifier\n",
      "RandomForestClassifier\n",
      "XGBClassifier\n",
      "RidgeClassifier\n"
     ]
    }
   ],
   "source": [
    "# best models names\n",
    "models_names = []\n",
    "for model in best_models:\n",
    "    print(model.__class__.__name__)\n",
    "    models_names.append(model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.359822</td>\n",
       "      <td>0.929969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RidgeClassifier</td>\n",
       "      <td>0.765090</td>\n",
       "      <td>0.878326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.858253</td>\n",
       "      <td>0.849504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.859273</td>\n",
       "      <td>0.847679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.816426</td>\n",
       "      <td>0.837376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.777901</td>\n",
       "      <td>0.795253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model     score  test_score\n",
       "2  DecisionTreeClassifier  0.359822    0.929969\n",
       "5         RidgeClassifier  0.765090    0.878326\n",
       "3  RandomForestClassifier  0.858253    0.849504\n",
       "4           XGBClassifier  0.859273    0.847679\n",
       "0      LogisticRegression  0.816426    0.837376\n",
       "1    KNeighborsClassifier  0.777901    0.795253"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame({'model': models_names, 'score': scores, 'test_score': test_scores})\n",
    "\n",
    "performance.sort_values(by='test_score', ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, min_samples_leaf=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, min_samples_leaf=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_leaf=2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=0, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=800, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n",
       "              feature_types=None, gamma=0, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=800, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='logloss',\n",
       "              feature_types=None, gamma=0, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.01, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=800, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random forest and XGBClassifier were the best, because the performe well both on inbalanced test data as well as balanced training data. ALso, the train and test socres are similar, with train score a little higher as espected (?).\n",
    "\n",
    "Finally, they beat the scoreline proposed for this project, which was 81%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving best models\n",
    "import pickle\n",
    "\n",
    "\n",
    "pickle.dump(best_models[3], open('../models/random_forest_classifier_model.pkl','wb'))\n",
    "\n",
    "file_name = 'XGBoost_model.pkl'\n",
    "pickle.dump(best_models[4], open('../models/'+file_name,'wb'))\n",
    "\n",
    "\n",
    "#loading best models\n",
    "rf = pickle.load(open('../models/random_forest_classifier_model.pkl','rb'))\n",
    "xgb = pickle.load(open('../models/'+file_name,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '../models/' + file_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/models/XGBoost_model.pkl'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
